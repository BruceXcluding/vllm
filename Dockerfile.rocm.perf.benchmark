# CONTEXT {'gpu_vendor': 'AMD', 'guest_os': 'UBUNTU'}
ARG BASE_DOCKER=rocm/pytorch:latest
FROM $BASE_DOCKER
USER root

# env
ENV WORKSPACE_DIR=/workspace
ENV MAX_JOBS=64
# vllm uses different env variables depending on branches. So just turn on all cases
ENV VLLM_USE_TRITON_FLASH_ATTN=1
ENV VLLM_USE_FLASH_ATTN_TRITON=1
ENV HIP_FORCE_DEV_KERNARG=0
ENV OPTIMIZE_EPILOGUE=1

# tunableOps
ENV PYTORCH_TUNABLEOP_ENABLED=0

WORKDIR $WORKSPACE_DIR

# torch
RUN pip install --upgrade pip
RUN pip install pandas

# TODO: remove when triton in BASE_DOCKER is updated to ver 2.3.0
ARG TRITON_COMMIT="bbe6246"
RUN pip uninstall triton -y && git clone https://github.com/triton-lang/triton.git && cd triton && git checkout ${TRITON_COMMIT} && cd python && python setup.py install

# vllm
RUN git clone https://github.com/ROCm/vllm.git -b perf_benchmark_navi &&\
    cd vllm &&\
    pip install -U -r requirements-rocm.txt &&\
    python3 setup.py install

# gradlib
RUN cd vllm/gradlib \
    && pip install .

RUN cd /opt/rocm/share/amd_smi &&\
    pip install .

# record configuration for posterity
RUN pip list
